{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/AUTALIC.csv\")\n",
    "\n",
    "# Define a function to determine the truth value based on annotator agreement\n",
    "def determine_truth(row):\n",
    "    scores = [row['A1_Score'], row['A2_Score'], row['A3_Score']]\n",
    "    # Count the occurrences of each score\n",
    "    counts = {score: scores.count(score) for score in set(scores)}\n",
    "    # Find the score that has two or more agreements\n",
    "    for score, count in counts.items():\n",
    "        if count >= 2:\n",
    "            return score\n",
    "    # If all three annotators disagree, return -1\n",
    "    return -1\n",
    "\n",
    "# Apply the function and create the 'truth' column\n",
    "df['truth'] = df.apply(determine_truth, axis=1)\n",
    "\n",
    "# Map the truth values to their corresponding string labels\n",
    "truth_mapping = {-1: \"Needs more context\", 0: \"Not ableist\", 1: \"Ableist\"}\n",
    "df['truth'] = df['truth'].map(truth_mapping)\n",
    "\n",
    "# Save the processed dataset\n",
    "# output_path = \"data/autalic.csv\"\n",
    "# df.to_csv(output_path, index=False)\n",
    "\n",
    "# print(f\"Processing complete. The output is saved as '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>llama3<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/autalic_test.csv\")\n",
    "\n",
    "# Initialize an empty list to store Llama 3's responses\n",
    "llama3_responses = []\n",
    "\n",
    "# Function to query Llama 3 using `ollama run` and enforce consistent short responses\n",
    "def ask_llama3(sentence):\n",
    "    prompt = (\n",
    "        f\"Answer whether the following sentence is ableist with only one of the following responses: \"\n",
    "        f\"'Ableist', 'Not ableist', or 'Needs more context'.\\n\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\"\n",
    "    )\n",
    "    try:\n",
    "        # Call the Ollama CLI using the `run` command\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"llama3\"],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "        )\n",
    "        \n",
    "        # Get the output and strip any extra whitespace\n",
    "        response = result.stdout.strip()\n",
    "        \n",
    "        # Ensure the response is one of the valid categories\n",
    "        valid_responses = ['Ableist', 'Not ableist', 'Needs more context']\n",
    "        if response in valid_responses:\n",
    "            return response\n",
    "        else:\n",
    "            print(f\"Unexpected response: '{response}' for sentence: {sentence}\")\n",
    "            return \"Needs more context\"  # Default to 'Needs more context' if unexpected response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Llama 3 for sentence: {sentence}\\n{e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Ensure the 'outputs' folder exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Loop through each sentence and query Llama 3\n",
    "for idx, sentence in enumerate(df['Sentence'], 1):\n",
    "    print(f\"Processing sentence {idx}/{len(df)}: {sentence}\")\n",
    "    response = ask_llama3(sentence)\n",
    "    llama3_responses.append(response)\n",
    "\n",
    "# Add the responses to the dataframe\n",
    "df['llama3_response'] = llama3_responses\n",
    "\n",
    "# Map the truth and responses to numeric labels for metrics calculation\n",
    "label_mapping = {\"Needs more context\": 0, \"Not ableist\": 1, \"Ableist\": 2}\n",
    "df['truth_numeric'] = df['truth'].map(label_mapping)\n",
    "df['llama3_response_numeric'] = df['llama3_response'].map(label_mapping)\n",
    "\n",
    "# Drop rows with errors or unmatched responses\n",
    "df = df.dropna(subset=['truth_numeric', 'llama3_response_numeric'])\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(df['truth_numeric'], df['llama3_response_numeric'])\n",
    "recall = recall_score(df['truth_numeric'], df['llama3_response_numeric'], average='weighted')\n",
    "f1 = f1_score(df['truth_numeric'], df['llama3_response_numeric'], average='weighted')\n",
    "report = classification_report(df['truth_numeric'], df['llama3_response_numeric'], target_names=label_mapping.keys())\n",
    "\n",
    "# Save the updated dataset\n",
    "output_file = \"outputs/autalic_llama3_result.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")\n",
    "print(f\"\\nMetrics:\\nAccuracy: {accuracy:.4f}\\nRecall: {recall:.4f}\\nF1 Score: {f1:.4f}\")\n",
    "print(\"\\nDetailed Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>gpt-4<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key (load from environment or .env file)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Assumed to be set in environment variables\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/autalic_test.csv\")\n",
    "\n",
    "# Initialize an empty list to store GPT-4's responses\n",
    "gpt4_responses = []\n",
    "\n",
    "# Function to query GPT-4 using the OpenAI API and enforce consistent short responses\n",
    "def ask_gpt4(sentence):\n",
    "    prompt = (\n",
    "        f\"Answer whether the following sentence is ableist with only one of the following responses: \"\n",
    "        f\"'Ableist', 'Not ableist', or 'Needs more context'.\\n\"\n",
    "        f\"Sentence: \\\"{sentence}\\\"\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Use the correct method `openai.ChatCompletion.create()` to interact with GPT-4\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=10,  # Keep response short\n",
    "            temperature=0,  # Make the output more deterministic\n",
    "            top_p=1,  # Use the top token probability\n",
    "            n=1,  # Only one response\n",
    "            stop=None  # Don't use a stop sequence\n",
    "        )\n",
    "        \n",
    "        # Get the model's response and strip extra whitespace\n",
    "        model_response = response['choices'][0]['message']['content'].strip()\n",
    "        \n",
    "        # Ensure the response is one of the valid categories\n",
    "        valid_responses = ['Ableist', 'Not ableist', 'Needs more context']\n",
    "        if model_response in valid_responses:\n",
    "            return model_response\n",
    "        else:\n",
    "            print(f\"Unexpected response: '{model_response}' for sentence: {sentence}\")\n",
    "            return \"Needs more context\"  # Default to 'Needs more context' if unexpected response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error querying GPT-4 for sentence: {sentence}\\n{e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "# Ensure the 'outputs' folder exists\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Loop through each sentence and query GPT-4\n",
    "for idx, sentence in enumerate(df['Sentence'], 1):\n",
    "    print(f\"Processing sentence {idx}/{len(df)}: {sentence}\")\n",
    "    response = ask_gpt4(sentence)\n",
    "    gpt4_responses.append(response)\n",
    "\n",
    "# Add the responses to the dataframe\n",
    "df['gpt4_response'] = gpt4_responses\n",
    "\n",
    "# Save the updated dataset\n",
    "output_file = \"outputs/autalic_gpt4_result.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Processing complete. Results saved to {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
